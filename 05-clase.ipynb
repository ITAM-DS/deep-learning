{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Multicapa\n",
    "\n",
    "Son modelos de alta capacidad,esto quiere decir que son flexibles. Cuando entrenamos modelos de alta capacidad corremos el riesgo de sobreajustar.\n",
    "\n",
    "### Perceptrón\n",
    "Es un algoritmo para el aprendizaje supervisado de clasificadores binarios, su objetivo es encontrar un hiperplano capaz de separar correctamente un conjunto de datos que sean linealmente separables.\n",
    "\n",
    "> Que dos grupos de observaciones sean linealmente separables significa que existe al menos un hiperplano (subespacio con una dimensión menor que el espacio que lo rodea) que permite separar perfectamente los dos grupos.\n",
    "\n",
    "En un espacio de dos dimensiones, cada observación está definida por un vector $x$ y una variable respuesta que puede tomar dos valores (-1, +1), para clasificar las observaciones se busca una función $\\sigma$ tal que:  \n",
    "\n",
    "$$\\sigma(x)=\\left\\lbrace\\begin{array}{c}1~~si~x\\geq0 \\\\ -1~~si~x<0  \\end{array}\\right.$$\n",
    "\n",
    "Es decir, $$\\sigma(x)= signo(x)$$\n",
    "\n",
    "El algoritmo del perceptrón es un algoritmo iterativo, es decir, propone soluciones secuenciales hasta llegar a un punto de terminación (óptimo).   \n",
    "\n",
    "$$w^{(1)},w^{(2)},w^{(3)},...,w^{(T)}$$\n",
    "\n",
    "$$w^{(T)}\\approx w^*$$\n",
    "\n",
    "Iniciamos en $w^{(i)}=0$ y para cada iteración $t$\n",
    "\n",
    "1. Buscamos elementos mal clasificados   \n",
    "\n",
    "$$y^{(i)}\\hat{y}^{(i)}=y^{(i)}\\langle w^{(t)},x^{(i)}\\rangle <0$$\n",
    "\n",
    "donde $i=1,...,n$\n",
    "\n",
    "2. Actualizamos   \n",
    "\n",
    "$$w^{(t+1)}=w^{(t)} + \\xi^{(i)}$$\n",
    "\n",
    "Repetimos los dos pasos anteriores hasta que todas las observaciones estén bien clasificadas.\n",
    "\n",
    "Este algoritmo puede tardar mucho de debido a:   \n",
    "\n",
    "* *Margen de separción*. Distancia entre el hiperplano y los casos límites de cada clase. \n",
    "\n",
    "\n",
    "* *Concentración de los datos*.Dado que la actualización es en términos de sumas y restas de los vectores $x's$, con componentes de $x's$ muy grandes va a hacer cambios muy agresivos y si hay datos muy separados el algoritmo puede estacionarse y no terminar.   \n",
    "\n",
    "El algoritmo de aprendizaje del perceptrón no termina si el conjunto de aprendizaje no se puede separar linealmente . Si los vectores no son linealmente separables, el aprendizaje nunca llegará a un punto en el que todos los vectores se clasifiquen correctamente.   \n",
    "\n",
    "### Extensiones de modelos lineales\n",
    "\n",
    "La linealidad asume un principio de monotonía, cualquier aumento en nuestra característica debe causar siempre un aumento en la salida de nuestro modelo (si el peso correspondiente es positivo), o siempre causar una disminución en la salida de nuestro modelo (si el peso correspondiente es negativo). \n",
    "\n",
    "Para romper el supueto de linealidad es necesario encadenar transformaciones lineales.\n",
    "\n",
    "Pensemos que representamos los datos con una matriz $nxd$\n",
    "\n",
    "$$X\\in\\mathbb{R}^{nxd}$$\n",
    "\n",
    "tenemos una capa uculta con $m$ unidades ocultas denotada por\n",
    "\n",
    "$$H\\in\\mathbb{R}^{nxm}$$\n",
    "\n",
    "Dado que las capas oculta y de salida están completamente conectadas, calculamos las salidas de la siguiente manera:\n",
    "\n",
    "$$\\underbrace{H}_{nxm}=\\underbrace{X}_{nxd}\\underbrace{W^{(1)}}_{dxm}+\\underbrace{b^{(1)}}_{mx1}$$\n",
    "\n",
    "$$\\underbrace{O}_{nxq}=\\underbrace{H}_{nxm}\\underbrace{W^{(2)}}_{mxq}+\\underbrace{b^{(2)}}_{qx1}$$\n",
    "\n",
    "$$=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}$$\n",
    "\n",
    "$$=XW^{(1)}W^{(2)}+b^{(1)})W^{(2)}+b^{(2)}$$\n",
    "\n",
    "$$=XW+b$$\n",
    "\n",
    "Lo que nos lleva a una función lineal.\n",
    "\n",
    "Necesitamos definir una función $H$ como una transformación lineal a la que será una función de activación no lineal σ. La contribución de $X$ a $O$ es no lineal por lo que rompe con la monotonía.\n",
    "\n",
    "$$H=\\sigma(XW^{(1)}+b^{(1)}),$$\n",
    "\n",
    "$$O=HW^{(2)}+b^{(2)}$$\n",
    "\n",
    "Encadenar transformaciones lineales nos permite hacer una composición de funciones que se tan compleja como queremos.\n",
    "\n",
    "Queremos que todas las capas intermedias hagan una buena transformación de los datos para que al final la última capa sea una combinación lineal de los atributos ficticios que se construyeron en el intermedio. Así esperamos un buen aproximador.\n",
    "\n",
    "### Teoremas de aproximación universal\n",
    "\n",
    "Para ciertas elecciones de la función de activación, los modelos multicapa son aproximadores universales. Nos ayudan a entender los modelos multicapa con transformaciones no lineales como familias de funciones suficientemente fexibles para poder aproximar cualquier función.\n",
    "\n",
    "### Funciones de activación\n",
    "\n",
    "Son operadores diferenciables para transformar señales de entrada en salidas (emulan el paso de una señal), son entrenables por métodos de optimización numérica. Pueden ser usados como factores de decisión, deciden si una neurona debe activarse o no calculando la suma ponderada y agregando más sesgo con ella.\n",
    "\n",
    "#### ReLU\n",
    "\n",
    "ReLU proporciona una transformación no lineal muy simple.\n",
    "\n",
    "$$ReLU(x)= máx(0,x)$$\n",
    "\n",
    "$$\\frac{\\partial ReLU(x)}{\\partial x}=\\left\\lbrace\\begin{array}{c}0~~si~x\\leq0 \\\\ 1~~si~x>0  \\end{array}\\right.$$\n",
    " \n",
    "De manera informal, la función ReLU retiene solo elementos positivos y descarta todos los elementos negativos estableciendo las activaciones correspondientes en 0.\n",
    "\n",
    "#### pReLU\n",
    "\n",
    "Una variante de la función ReLU es ReLU parametrizada (pReLU). Esta variación agrega un término lineal a ReLU, por lo que parte de la información aún se transmite, incluso cuando el argumento es negativo.\n",
    "\n",
    "$$pReLU(x)= máx(0,x)+ \\alpha min(0,x)$$\n",
    "\n",
    "$$\\frac{\\partial pReLU(x)}{\\partial x}=\\left\\lbrace\\begin{array}{c}\\alpha~~si~x\\leq0 \\\\ 1~~si~x>0  \\end{array}\\right.$$\n",
    "\n",
    "#### Sigmoide\n",
    "\n",
    "La función sigmoide transforma sus entradas que se encuentran en el dominio $\\mathbb{R}$, en salidas que se encuentran en el intervalo (0, 1). Aplasta cualquier entrada en el rango (-inf, inf) a algún valor en el rango (0, 1):\n",
    "\n",
    "$$\\sigma(x)= \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$\\frac{\\partial \\sigma(x)}{\\partial x}=\\frac{e^{-x}}{{(1+e^{-x})}^{2}}= \\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "La derivada de la función sigmoide alcanza un máximo de 0.25 y a medida que la entrada diverge de 0 en cualquier dirección, la derivada se acerca a 0.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
